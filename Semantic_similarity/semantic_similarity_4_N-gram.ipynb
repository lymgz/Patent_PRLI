{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, merge. 1.\n",
    "1. The two files are named \"dictionary 1-robot.xlsx\" and \"dictionary 2non-robot.xlsx\". \n",
    "2. Please merge sheet2 of the two tables into a new table according to the following rules: 2.1.\n",
    "2.1. The structure of the two tables is exactly the same, with the headers \"Patent #\" \"publication year\" \"keyword from n-gram\", please merge the two tables into a single table.\n",
    "2.2. before merging, iterate over \"patent #\" if the two tables have identical \"patent #\" then, \"publication year\" keep one of them, \"keyword from n-gram\" merge it and space it with \";\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged table saved as 'merged_table.xlsx'\n",
      "Number of rows in the merged table: 779 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Eric\\.conda\\envs\\lym1\\lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file names\n",
    "file1 = \"dictionary 1-robot.xlsx\"\n",
    "file2 = \"dictionary 2non-robot.xlsx\"\n",
    "\n",
    "# Load sheet2 from both Excel files\n",
    "df1 = pd.read_excel(file1, sheet_name=\"sheet2\")\n",
    "df2 = pd.read_excel(file2, sheet_name=\"sheet2\")\n",
    "\n",
    "# Merge the two DataFrames based on the \"Patent #\" column\n",
    "merged_df = pd.merge(df1, df2, on=\"Patent #\", how=\"outer\")\n",
    "\n",
    "# Define a function to merge the \"keyword from n-gram\" columns with \";\"\n",
    "def merge_keywords(row):\n",
    "    keywords1 = str(row[\"keyword from n-gram_x\"])\n",
    "    keywords2 = str(row[\"keyword from n-gram_y\"])\n",
    "    if keywords1 == \"nan\":\n",
    "        return keywords2\n",
    "    elif keywords2 == \"nan\":\n",
    "        return keywords1\n",
    "    else:\n",
    "        return keywords1 + \"; \" + keywords2\n",
    "\n",
    "# Apply the function to merge the \"keyword from n-gram\" columns\n",
    "merged_df[\"keyword from n-gram\"] = merged_df.apply(merge_keywords, axis=1)\n",
    "\n",
    "# Drop the redundant columns and keep one \"publication year\" and \"PRLI\"\n",
    "merged_df = merged_df.drop(columns=[\"publication year_y\", \"PRLI_y\", \"keyword from n-gram_x\", \"keyword from n-gram_y\"])\n",
    "\n",
    "# Rename the columns if needed\n",
    "# merged_df = merged_df.rename(columns={\"publication year_x\": \"publication year\", \"PRLI_x\": \"PRLI\"})\n",
    "\n",
    "# Save the merged DataFrame to a new Excel file\n",
    "merged_df.to_excel(\"merged_table.xlsx\", index=False)\n",
    "\n",
    "print(\"Merged table saved as 'merged_table.xlsx'\")\n",
    "print(\"Number of rows in the merged table:\", len(merged_df), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preprocess \"merged_table.xlsx\":\n",
    "\n",
    "Read \"merged_table.xlsx.\"\n",
    "Process the \"keyword from n-gram\" column to replace underscores with spaces and split multiple keywords separated by ';' into separate rows.\n",
    "Create a new dataset called \"processed_keyword.\"\n",
    "### Step 2: Preprocess \"corpus_dataset.xlsx\":\n",
    "\n",
    "Read \"corpus_dataset.xlsx.\"\n",
    "Process the \"Corpus\" column to split words, remove stopwords, and perform common text cleaning.\n",
    "Create a new dataset called \"processed_corpus.\"\n",
    "### Step 3: Calculate Semantic Similarity:\n",
    "\n",
    "Calculate semantic similarity between \"processed_keyword\" and \"processed_corpus\" using cosine similarity, Jaccard similarity, and Euclidean distance.\n",
    "Output the similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correspondence Check Results:\n",
      "         Patent ID                           Processed Keywords  \\\n",
      "0     FI198303365A                        articulated robot arm   \n",
      "1      FR2547755A1               lightweight construction robot   \n",
      "2       EP132176A1               lightweight construction robot   \n",
      "3      JP60087983A               lightweight construction robot   \n",
      "4    ES198503542A1               lightweight construction robot   \n",
      "..             ...                                          ...   \n",
      "774   CN218534534U                       moving track mechanism   \n",
      "775   CN115599099A                      robot coordinate system   \n",
      "776   CN115653251A  speed reducing motor vertical sliding plate   \n",
      "777  KR2023022067A                   three dimensional printing   \n",
      "778   CN115726591A                         wall building device   \n",
      "\n",
      "                                 Processed Corpus Text  \n",
      "0    articulated rotary robot arm hinge coupling li...  \n",
      "1    prim advantage invention light weight thus mou...  \n",
      "2    prim advantage invention light weight thus mou...  \n",
      "3    prim advantage invention light weight thus mou...  \n",
      "4    prim advantage invention light weight thus mou...  \n",
      "..                                                 ...  \n",
      "774  lifting movable building construction robot re...  \n",
      "775  autonomous navigation robot used military indu...  \n",
      "776  building robot indoor spraying used indoor wal...  \n",
      "777  method constructing reinforced bar combination...  \n",
      "778  wall building device use building construction...  \n",
      "\n",
      "[779 rows x 3 columns]\n",
      "Are the Common Patent IDs Unique? True\n",
      "Number of Common Patent IDs: 779\n",
      "Number of Redundant Processed Corpus Text Entries: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Preprocess \"merged_table.xlsx\"\n",
    "merged_data = pd.read_excel(\"merged_table.xlsx\")\n",
    "\n",
    "# Create a dictionary to store processed keywords\n",
    "keyword_dict = {}\n",
    "\n",
    "for index, row in merged_data.iterrows():\n",
    "    patent_id = row[\"Patent #\"]\n",
    "    keywords = row[\"keyword from n-gram\"].split(\";\")\n",
    "    \n",
    "    # Process and clean the keywords\n",
    "    cleaned_keywords = []\n",
    "    for keyword in keywords:\n",
    "        keyword = keyword.replace(\"_\", \" \").strip()\n",
    "        keyword = keyword.replace(\"robots\", \"robot\").strip()\n",
    "        cleaned_keywords.append(keyword)\n",
    "    \n",
    "    # Store the processed keywords for each patent ID\n",
    "    keyword_dict[patent_id] = \" \".join(cleaned_keywords)\n",
    "\n",
    "# Step 2: Preprocess \"corpus_dataset.xlsx\"\n",
    "corpus_data = pd.read_excel(\"corpus_dataset.xlsx\")\n",
    "\n",
    "# Create a dictionary to store processed corpus text\n",
    "corpus_dict = {}\n",
    "\n",
    "for index, row in corpus_data.iterrows():\n",
    "    patent_id = row[\"Patent #\"]\n",
    "    text = row[\"Corpus\"]\n",
    "    \n",
    "    # Process and clean the corpus text\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stopwords.words('english')]\n",
    "        cleaned_text = \" \".join(tokens)\n",
    "        corpus_dict[patent_id] = cleaned_text\n",
    "\n",
    "# Filter the corpus dictionary to keep only entries with matching patent IDs in keyword_dict\n",
    "corpus_dict = {patent_id: text for patent_id, text in corpus_dict.items() if patent_id in keyword_dict}\n",
    "\n",
    "# Check if there are any redundant Processed Corpus Text entries\n",
    "redundant_corpus_text = [patent_id for patent_id in corpus_dict if patent_id not in keyword_dict]\n",
    "\n",
    "# Create a table to display Patent ID, Processed Keywords, Processed Corpus Text\n",
    "table_data = []\n",
    "for patent_id in keyword_dict:\n",
    "    if patent_id in corpus_dict:\n",
    "        table_data.append([patent_id, keyword_dict[patent_id], corpus_dict[patent_id]])\n",
    "\n",
    "# Display the table\n",
    "table = pd.DataFrame(table_data, columns=[\"Patent ID\", \"Processed Keywords\", \"Processed Corpus Text\"])\n",
    "\n",
    "# Display the results\n",
    "print(\"Correspondence Check Results:\")\n",
    "print(table)\n",
    "\n",
    "# Ensure that the lengths of processed_keyword and processed_corpus are the same\n",
    "common_patent_ids = set(keyword_dict.keys()).intersection(corpus_dict.keys())\n",
    "\n",
    "# Check the number of common patent IDs and their uniqueness\n",
    "print(f\"Are the Common Patent IDs Unique? {len(common_patent_ids) == len(set(common_patent_ids))}\")\n",
    "print(f\"Number of Common Patent IDs: {len(common_patent_ids)}\")\n",
    "print(f\"Number of Redundant Processed Corpus Text Entries: {len(redundant_corpus_text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 3: Calculate Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE want to compare the cosine similarity, Euclidean distance, and Jaccard similarity between the long sentences formed by combining all \"Processed Keywords\" and all \"Processed Corpus Text\" values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Score: 0.9147749990130831\n",
      "Euclidean Distance Score: 0.412855909457324\n",
      "Jaccard Similarity Score: 0.1185076810534016\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "# Combine all \"Processed Keywords\" into a single long sentence\n",
    "all_keywords = \" \".join(table[\"Processed Keywords\"])\n",
    "\n",
    "# Combine all \"Processed Corpus Text\" into a single long sentence\n",
    "all_corpus_text = \" \".join(table[\"Processed Corpus Text\"])\n",
    "\n",
    "# Step 3: Calculate Semantic Similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_keyword = tfidf_vectorizer.fit_transform([all_keywords])\n",
    "tfidf_matrix_corpus = tfidf_vectorizer.transform([all_corpus_text])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarity_scores = cosine_similarity(tfidf_matrix_keyword, tfidf_matrix_corpus)[0][0]\n",
    "\n",
    "# Calculate Euclidean distance\n",
    "euclidean_distance_scores = euclidean_distances(tfidf_matrix_keyword, tfidf_matrix_corpus)[0][0]\n",
    "\n",
    "# Calculate Jaccard similarity\n",
    "keywords_set = set(all_keywords.split())\n",
    "corpus_set = set(all_corpus_text.split())\n",
    "\n",
    "# Calculate Jaccard similarity\n",
    "jaccard_similarity = len(keywords_set.intersection(corpus_set)) / len(keywords_set.union(corpus_set))\n",
    "\n",
    "# Output the similarity scores\n",
    "print(\"Cosine Similarity Score:\", cosine_similarity_scores)\n",
    "print(\"Euclidean Distance Score:\", euclidean_distance_scores)\n",
    "print(\"Jaccard Similarity Score:\", jaccard_similarity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lym1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
